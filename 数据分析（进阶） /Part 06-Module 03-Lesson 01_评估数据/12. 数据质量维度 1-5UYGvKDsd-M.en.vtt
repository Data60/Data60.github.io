WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.765
Every dirty dataset is dirty in its own unique way.

00:00:03.765 --> 00:00:09.675
Trying to list every specific quality issue here in this lesson is, therefore futile.

00:00:09.675 --> 00:00:11.460
But we can categorize them.

00:00:11.460 --> 00:00:15.795
These categories of data quality are called data quality dimensions.

00:00:15.794 --> 00:00:20.894
But there's little agreement among researchers as to what these dimensions actually are,

00:00:20.894 --> 00:00:23.285
or even if they should be called dimensions.

00:00:23.285 --> 00:00:25.920
For example, this source has six dimensions.

00:00:25.920 --> 00:00:27.870
This source calls them data validity

00:00:27.870 --> 00:00:31.620
rules and actually has some different terms compared to the first source.

00:00:31.620 --> 00:00:35.375
This source calls them data quality aspects and lists eight.

00:00:35.375 --> 00:00:38.645
And this source says there are seven dimensions.

00:00:38.645 --> 00:00:41.990
So some sources list more, others less.

00:00:41.990 --> 00:00:43.250
Some call them aspects,

00:00:43.250 --> 00:00:46.189
metrics, rules, measures, dimensions.

00:00:46.189 --> 00:00:48.344
There's no real solid standard.

00:00:48.344 --> 00:00:51.909
Everyone does seem to agree on these next four though.

00:00:51.909 --> 00:00:55.204
And the vast majority of data quality issues that I've come across,

00:00:55.204 --> 00:00:58.759
can be sorted into these four buckets: completeness,

00:00:58.759 --> 00:01:01.519
validity, accuracy, and consistency.

00:01:01.520 --> 00:01:05.120
These dimensions can help guide your thoughts when assessing your data.

00:01:05.120 --> 00:01:09.620
These are listed in decreasing order of severity which will make sense shortly.

00:01:09.620 --> 00:01:11.075
So completeness.

00:01:11.075 --> 00:01:13.605
Do we have all of the records that we should?

00:01:13.605 --> 00:01:15.350
Do we have missing records or not?

00:01:15.349 --> 00:01:16.459
Are there specific rows,

00:01:16.459 --> 00:01:18.454
columns, or cells missing?

00:01:18.454 --> 00:01:23.840
An example of this in our dataset for this lesson is the missing hba1c change variable,

00:01:23.840 --> 00:01:25.670
represented by NaN's here.

00:01:25.670 --> 00:01:27.879
Next, validity.

00:01:27.879 --> 00:01:30.719
We have the records but they are not valid.

00:01:30.719 --> 00:01:34.980
More technically, it doesn't conform to a defined schema.

00:01:34.980 --> 00:01:38.704
First of all, a schema is basically a defined set of rules for data.

00:01:38.704 --> 00:01:42.024
This schema can be enforced by real world constraints,

00:01:42.025 --> 00:01:44.010
like you can't have negative height.

00:01:44.010 --> 00:01:47.270
You can't be negative 66 inches tall for example.

00:01:47.269 --> 00:01:49.729
These individuals aren't in this dataset, but just imagine.

00:01:49.730 --> 00:01:54.734
But this schema can also be a specific schema just for your table or database.

00:01:54.734 --> 00:01:57.575
A good example of this is a primary key in a database,

00:01:57.575 --> 00:01:59.703
or unique key constraints in tables.

00:01:59.703 --> 00:02:03.924
In this dataset, that means there can't be duplicate patient IDs.

00:02:03.924 --> 00:02:06.560
If systems are good, they aren't always though,

00:02:06.560 --> 00:02:09.120
invalid record creation will be denied.

00:02:09.120 --> 00:02:10.480
For example, in this dataset,

00:02:10.479 --> 00:02:13.639
if you try to create another patient with the patient ID of one,

00:02:13.639 --> 00:02:16.024
you'd be denied, ideally at least.

00:02:16.025 --> 00:02:20.689
Another example in this dataset of invalid data is our zip code data here.

00:02:20.689 --> 00:02:24.889
This is invalidated because zip codes must be five digits in an integer.

00:02:24.889 --> 00:02:29.439
Whereas this one is actually a float and sometimes four digits.

00:02:29.439 --> 00:02:30.965
Now, accuracy.

00:02:30.965 --> 00:02:34.939
Inaccurate data is wrong data that is valid.

00:02:34.939 --> 00:02:38.824
It adheres to the defined schema, but it's still incorrect.

00:02:38.824 --> 00:02:42.109
This data doesn't conform to a gold standard, if you will.

00:02:42.110 --> 00:02:44.855
For example, looking at weight in this table.

00:02:44.854 --> 00:02:48.439
Imagine at the scale that each patient was measured on was slightly

00:02:48.439 --> 00:02:52.609
faulty and overestimated each patient's weight by five pounds.

00:02:52.610 --> 00:02:55.595
The weight's slightly off, but still valid.

00:02:55.594 --> 00:02:58.159
Inaccuracy issue that we already identified was

00:02:58.159 --> 00:03:01.745
the height entry for this patient being 27 inches.

00:03:01.745 --> 00:03:04.640
27 inches is technically still valid.

00:03:04.639 --> 00:03:08.089
It's possible for an adult human to be 27 inches tall.

00:03:08.090 --> 00:03:11.659
The shortest adult ever measured 21 inches approximately.

00:03:11.659 --> 00:03:13.370
And since the inclusion criteria for

00:03:13.370 --> 00:03:16.745
this clinical trial required patients to be at least 18 years old,

00:03:16.745 --> 00:03:19.969
we can ignore the possibility of this being a child's height.

00:03:19.969 --> 00:03:22.805
So again, 27 inches is possible, but it's unlikely.

00:03:22.805 --> 00:03:24.890
And we already know that the correct height is actually

00:03:24.889 --> 00:03:27.619
72 inches and that these digits were switched around.

00:03:27.620 --> 00:03:29.620
So this is an accurate data.

00:03:29.620 --> 00:03:31.819
And finally, consistency.

00:03:31.819 --> 00:03:35.264
Inconsistent data is both valid and accurate.

00:03:35.264 --> 00:03:38.464
But there are multiple correct ways of referring to the same thing.

00:03:38.465 --> 00:03:40.879
We want consistency in fields that represent

00:03:40.879 --> 00:03:44.269
the same data across tables or within tables.

00:03:44.270 --> 00:03:47.100
We're looking for a standard format basically.

00:03:47.099 --> 00:03:50.944
In this dataset, we've identified inconsistent representations of state,

00:03:50.944 --> 00:03:53.439
full state name versus abbreviation.

00:03:53.439 --> 00:03:56.344
For example, California and CA,

00:03:56.344 --> 00:03:58.550
or New York and NY.

00:03:58.550 --> 00:04:00.195
That's inconsistent.

00:04:00.194 --> 00:04:04.174
This inconsistency means we can't analyze our dataset based on state.

00:04:04.175 --> 00:04:06.230
At least until we clean this issue.

00:04:06.229 --> 00:04:09.284
So those are the four main dimensions of data quality.

00:04:09.284 --> 00:04:12.604
Again, the order of these here is in decreasing order of severity.

00:04:12.604 --> 00:04:15.439
Completeness issues means we don't have the data.

00:04:15.439 --> 00:04:16.579
Valid data exist but,

00:04:16.579 --> 00:04:18.094
it doesn't adhere to a schema.

00:04:18.095 --> 00:04:21.178
Inaccurate data is present and valid but incorrect.

00:04:21.177 --> 00:04:23.855
And inconsistent data is present valid and accurate,

00:04:23.855 --> 00:04:26.030
but it has multiple representations.

00:04:26.029 --> 00:04:27.859
Again, you can use these dimensions to help

00:04:27.860 --> 00:04:30.230
guide your thought process when assessing your data.

